{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SIMPLE RNN with Word2Vec**"
      ],
      "metadata": {
        "id": "A8lRTu3br7KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "mPVUMP6YuQjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as ex\n",
        "import plotly.figure_factory as ff"
      ],
      "metadata": {
        "id": "fsyFPyiBuHh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('mount success')"
      ],
      "metadata": {
        "id": "Oz9RJ8lJsZJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/Shareddrives/EMOThreat/train_set_taskA.csv')\n",
        "test = pd.read_csv('/content/drive/Shareddrives/EMOThreat/test_set_taskA.csv')\n",
        "train.head()\n",
        "test.head()"
      ],
      "metadata": {
        "id": "66o0YbjxsYlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "id": "LerZp_ehsqzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning"
      ],
      "metadata": {
        "id": "JXpsDhNFs51E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re,string\n",
        "\n",
        "def strip_links(text):\n",
        "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
        "    links         = re.findall(link_regex, text)\n",
        "    for link in links:\n",
        "        text = text.replace(link[0], ', ')    \n",
        "    return text\n",
        "train['Sentences']=train['Sentences'].apply(lambda x:strip_links(x))\n",
        "test['Sentences']=test['Sentences'].apply(lambda x:strip_links(x))\n",
        "### replace :\\n \n",
        "train['Sentences']=train['Sentences'].str.replace(\"\\n\",' ')\n",
        "### replace :\\n \n",
        "test['Sentences']=test['Sentences'].str.replace(\"\\n\",' ')\n",
        "# Define the function to remove the punctuation\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "# Apply to the DF series\n",
        "train['Sentences'] = train['Sentences'].apply(remove_punctuations) \n",
        "# Apply to the DF series\n",
        "test['Sentences'] = test['Sentences'].apply(remove_punctuations) "
      ],
      "metadata": {
        "id": "RWqWAE4Ts657"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "2NRrNFeDsLWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_classes = [\"anger\",\"disgust\",\"fear\",\"sadness\",\"surprise\",\"happiness\",\"neutral\"]\n",
        "Y = train[list_classes].values"
      ],
      "metadata": {
        "id": "chA-R54mkSGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train.Sentences.values, Y,  \n",
        "                                                  random_state=42, \n",
        "                                                  test_size=0.2)"
      ],
      "metadata": {
        "id": "dwchLlDfkSER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRK_5omNkOnw"
      },
      "outputs": [],
      "source": [
        "train['Sentences'].apply(lambda x:len(str(x).split())).max()\n",
        "max_features = 5000\n",
        "maxlen = 500\n",
        "token=tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
        "token.fit_on_texts(train.Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_seq=token.texts_to_sequences(X_train)\n",
        "X_test_seq=token.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "66_cGmfovbiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zero pad the sequences\n",
        "X_train_pad = sequence.pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = sequence.pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "word_index = token.word_index\n",
        "len(token.word_index)"
      ],
      "metadata": {
        "id": "w0Haxxj2wgDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "!unzip \"/content/drive/Shareddrives/EMOThreat/urdu-w2vec.zip\" -d \"/content/\"\n",
        "word2vec_model = gensim.models.Word2Vec.load(\"/content/urdu-w2vec\")\n",
        "\n",
        "print(word2vec_model)"
      ],
      "metadata": {
        "id": "trt1s0Ttt6IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vector_length = 100\n",
        "#Initialize embedding matrix\n",
        "embedding_matrix = np.zeros((max_features + 1, embedding_vector_length))\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "\n",
        "for word, i in sorted(token.word_index.items(),key=lambda x:x[1]):\n",
        "    if i > (max_features+1):\n",
        "        break\n",
        "    try:\n",
        "        embedding_vector = word2vec_model[word] #Reading word's embedding from Glove model for a given word\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except:\n",
        "        pass\n",
        "embedding_matrix"
      ],
      "metadata": {
        "id": "h2RgSW2nuUGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building Using Word2vec"
      ],
      "metadata": {
        "id": "eAlOKG6FufUl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nPztv5ndvfN"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(token.word_index) + 1\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "deep_inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = Embedding(5001, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
        "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
        "dense_layer_1 = Dense(7, activation='sigmoid')(LSTM_Layer_1)\n",
        "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsTgamxpfHNa"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize model\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "# A simpleRNN without any pretrained embeddings and one dense layer\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_features + 1, #Vocablury size\n",
        "                                    embedding_vector_length, #Embedding size\n",
        "                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n",
        "                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n",
        "                                    input_length=maxlen) #Number of words in each review\n",
        "         )\n",
        "model.add(SimpleRNN(100))\n",
        "model.add(Dense(7, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "x5Y8TCj0ugbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))"
      ],
      "metadata": {
        "id": "o4wN6zd4vjIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test_pad, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "cE4wzuuDxYcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "test['Sentences'].apply(lambda x:len(str(x).split())).max()\n",
        "max_features = 5000\n",
        "maxlen = 500\n",
        "token=tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
        "token.fit_on_texts(test.Sentences)\n",
        "\n",
        "X = test[\"Sentences\"]\n",
        "X_test_seq=token.texts_to_sequences(X)\n",
        "#zero pad the sequences\n",
        "X_test_pad = sequence.pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "word_index = token.word_index\n",
        "len(token.word_index)\n",
        "\n",
        "results=model.predict(X_test_pad)\n",
        "res = results.round()\n",
        "res= res.astype(int)\n",
        "csv = pd.DataFrame(res.tolist(),columns=[\"anger\",\"disgust\",\"fear\",\"sadness\",\"surprise\",\"happiness\",\"neutral\"])\n",
        "csv.insert(7,\"Sentences\",test[\"Sentences\"],True)\n",
        "csv.to_csv(\"word2vec.csv\", index=False) "
      ],
      "metadata": {
        "id": "Bbze5YB9jUuD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}